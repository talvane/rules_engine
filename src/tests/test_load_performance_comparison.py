#!/usr/bin/env python3
"""
Teste de Carga: Compara√ß√£o de Performance Sync vs Async
Avalia performance sob diferentes cargas de trabalho e cen√°rios
"""

import sys
import os
import time
import asyncio
import random
import statistics
from typing import Dict, List, Tuple
from concurrent.futures import ThreadPoolExecutor, as_completed

# Adiciona o path para importar o m√≥dulo
sys.path.insert(0, os.path.join(os.path.dirname(__file__), "..", ".."))

from src.lib.json_logic import jsonLogic, jsonLogicAsync, jsonLogicAuto


class LoadTest:
    """Teste de carga para JSON Logic"""

    def __init__(self):
        self.results = {}

    # ========== FUN√á√ïES DE APOIO ==========

    def calc_fibonacci(self, n):
        """C√°lculo CPU intensivo - Fibonacci"""
        if n <= 1:
            return n
        return self.calc_fibonacci(n - 1) + self.calc_fibonacci(n - 2)

    async def calc_fibonacci_async(self, n):
        """Vers√£o async do Fibonacci"""
        if n <= 1:
            return n
        # Simula pequena lat√™ncia entre c√°lculos
        await asyncio.sleep(0.0001)
        return await self.calc_fibonacci_async(n - 1) + await self.calc_fibonacci_async(n - 2)

    def db_query_simulation(self, query_id, complexity=1):
        """Simula consulta ao banco de dados"""
        # Simula lat√™ncia de rede/DB
        latency = random.uniform(0.001, 0.01) * complexity
        time.sleep(latency)
        
        # Simula processamento de dados
        result = query_id * random.randint(1, 100)
        for i in range(complexity * 10):
            result = (result * 1.1) % 10000
        
        return int(result)

    async def db_query_simulation_async(self, query_id, complexity=1):
        """Vers√£o async da simula√ß√£o de DB"""
        # Simula lat√™ncia async de rede/DB
        latency = random.uniform(0.001, 0.01) * complexity
        await asyncio.sleep(latency)
        
        # Simula processamento de dados
        result = query_id * random.randint(1, 100)
        for i in range(complexity * 10):
            result = (result * 1.1) % 10000
        
        return int(result)

    def process_text(self, text, pattern="test"):
        """Processamento de texto CPU intensivo"""
        result = 0
        words = text.split()
        
        for word in words:
            # Opera√ß√µes custosas com strings
            if pattern.lower() in word.lower():
                result += len(word) * ord(word[0])
            
            # Mais processamento
            for char in word:
                result += ord(char)
        
        return result % 10000

    async def process_text_async(self, text, pattern="test"):
        """Vers√£o async do processamento de texto"""
        result = 0
        words = text.split()
        
        for word in words:
            # Simula lat√™ncia entre processamentos
            await asyncio.sleep(0.0001)
            
            if pattern.lower() in word.lower():
                result += len(word) * ord(word[0])
            
            for char in word:
                result += ord(char)
        
        return result % 10000

    # ========== CEN√ÅRIOS DE TESTE ==========

    def get_light_load_scenario(self):
        """Cen√°rio de carga leve - opera√ß√µes simples"""
        async def add_async(a, b):
            return a + b
        
        async def multiply_async(a, b):
            return a * b
        
        async def power_async(a, b):
            return a ** b if b <= 10 else a * 10
        
        return {
            "name": "Light Load",
            "description": "Opera√ß√µes matem√°ticas b√°sicas",
            "iterations": 1000,
            "sync_functions": {
                "add": lambda a, b: a + b,
                "multiply": lambda a, b: a * b,
                "power": lambda a, b: a ** b if b <= 10 else a * 10
            },
            "async_functions": {
                "add": add_async,
                "multiply": multiply_async,
                "power": power_async
            },
            "test_data": [
                {"a": random.randint(1, 100), "b": random.randint(1, 10)}
                for _ in range(100)
            ],
            "rules": [
                {"apply": ["add", {"var": "a"}, {"var": "b"}]},
                {"apply": ["multiply", {"var": "a"}, {"var": "b"}]},
                {"apply": ["power", {"var": "a"}, 2]}
            ]
        }

    def get_medium_load_scenario(self):
        """Cen√°rio de carga m√©dia - opera√ß√µes com I/O simulado"""
        async def calc_fib_async(n):
            """Vers√£o async limitada do Fibonacci"""
            limited_n = min(n, 15)  # Limita para evitar explos√£o
            return self.calc_fibonacci(limited_n)
        
        return {
            "name": "Medium Load", 
            "description": "Simula√ß√£o de consultas DB + processamento",
            "iterations": 500,
            "sync_functions": {
                "db_query": self.db_query_simulation,
                "calc_fib": lambda n: self.calc_fibonacci(min(n, 15)),  # Limita para evitar explos√£o
                "process_data": self.process_text
            },
            "async_functions": {
                "db_query": self.db_query_simulation_async,
                "calc_fib": calc_fib_async,
                "process_data": self.process_text_async
            },
            "test_data": [
                {
                    "query_id": random.randint(1, 1000),
                    "fib_n": random.randint(5, 12),
                    "text": f"test data processing {random.randint(1, 100)} complex text analysis",
                    "complexity": random.randint(1, 3)
                }
                for _ in range(100)
            ],
            "rules": [
                {"apply": ["db_query", {"var": "query_id"}, {"var": "complexity"}]},
                {"apply": ["calc_fib", {"var": "fib_n"}]},
                {"apply": ["process_data", {"var": "text"}]}
            ]
        }

    def get_heavy_load_scenario(self):
        """Cen√°rio de carga pesada - opera√ß√µes intensivas"""
        complex_text = " ".join([
            f"complex processing test data item {i} with extensive content"
            for i in range(50)
        ])
        
        async def heavy_calc_async(a, b):
            """Vers√£o async do c√°lculo pesado"""
            return (a * b) % 10000 + sum(range(min(a, 100)))
        
        return {
            "name": "Heavy Load",
            "description": "Opera√ß√µes CPU/I/O intensivas",
            "iterations": 200,
            "sync_functions": {
                "heavy_db": lambda query_id, comp: self.db_query_simulation(query_id, comp),
                "heavy_text": lambda text: self.process_text(text),
                "heavy_calc": lambda a, b: (a * b) % 10000 + sum(range(min(a, 100)))
            },
            "async_functions": {
                "heavy_db": self.db_query_simulation_async,
                "heavy_text": self.process_text_async,
                "heavy_calc": heavy_calc_async
            },
            "test_data": [
                {
                    "query_id": random.randint(1, 10000),
                    "complexity": random.randint(3, 5),
                    "text": complex_text,
                    "a": random.randint(10, 200),
                    "b": random.randint(10, 200)
                }
                for _ in range(50)
            ],
            "rules": [
                {"apply": ["heavy_db", {"var": "query_id"}, {"var": "complexity"}]},
                {"apply": ["heavy_text", {"var": "text"}]},
                {"apply": ["heavy_calc", {"var": "a"}, {"var": "b"}]}
            ]
        }

    # ========== EXECU√á√ÉO DE TESTES ==========

    def measure_sync_performance(self, scenario: Dict) -> Dict:
        """Mede performance s√≠ncrona"""
        print(f"  üìä Testando SYNC - {scenario['name']}")
        
        times = []
        errors = 0
        
        start_total = time.perf_counter()
        
        for i in range(scenario['iterations']):
            data_idx = i % len(scenario['test_data'])
            rule_idx = i % len(scenario['rules'])
            
            data = scenario['test_data'][data_idx]
            rule = scenario['rules'][rule_idx]
            
            start = time.perf_counter()
            try:
                jsonLogic(rule, data, scenario['sync_functions'])
                end = time.perf_counter()
                times.append(end - start)
            except Exception as e:
                errors += 1
                end = time.perf_counter()
                times.append(end - start)
        
        end_total = time.perf_counter()
        total_time = end_total - start_total
        
        return {
            "total_time": total_time,
            "avg_time": statistics.mean(times) if times else 0,
            "median_time": statistics.median(times) if times else 0,
            "min_time": min(times) if times else 0,
            "max_time": max(times) if times else 0,
            "std_dev": statistics.stdev(times) if len(times) > 1 else 0,
            "throughput": scenario['iterations'] / total_time,
            "errors": errors,
            "success_rate": (scenario['iterations'] - errors) / scenario['iterations'] * 100
        }

    async def measure_async_performance(self, scenario: Dict) -> Dict:
        """Mede performance ass√≠ncrona"""
        print(f"  üìä Testando ASYNC - {scenario['name']}")
        
        times = []
        errors = 0
        
        start_total = time.perf_counter()
        
        for i in range(scenario['iterations']):
            data_idx = i % len(scenario['test_data'])
            rule_idx = i % len(scenario['rules'])
            
            data = scenario['test_data'][data_idx]
            rule = scenario['rules'][rule_idx]
            
            start = time.perf_counter()
            try:
                await jsonLogicAsync(rule, data, scenario['async_functions'])
                end = time.perf_counter()
                times.append(end - start)
            except Exception as e:
                errors += 1
                end = time.perf_counter()
                times.append(end - start)
        
        end_total = time.perf_counter()
        total_time = end_total - start_total
        
        return {
            "total_time": total_time,
            "avg_time": statistics.mean(times) if times else 0,
            "median_time": statistics.median(times) if times else 0,
            "min_time": min(times) if times else 0,
            "max_time": max(times) if times else 0,
            "std_dev": statistics.stdev(times) if len(times) > 1 else 0,
            "throughput": scenario['iterations'] / total_time,
            "errors": errors,
            "success_rate": (scenario['iterations'] - errors) / scenario['iterations'] * 100
        }

    async def measure_concurrent_async_performance(self, scenario: Dict, concurrency: int = 10) -> Dict:
        """Mede performance ass√≠ncrona com concorr√™ncia"""
        print(f"  üìä Testando ASYNC CONCURRENT ({concurrency}) - {scenario['name']}")
        
        async def run_batch(batch_size: int):
            tasks = []
            for i in range(batch_size):
                data_idx = i % len(scenario['test_data'])
                rule_idx = i % len(scenario['rules'])
                
                data = scenario['test_data'][data_idx]
                rule = scenario['rules'][rule_idx]
                
                task = jsonLogicAsync(rule, data, scenario['async_functions'])
                tasks.append(task)
            
            start = time.perf_counter()
            try:
                results = await asyncio.gather(*tasks, return_exceptions=True)
                end = time.perf_counter()
                
                errors = sum(1 for r in results if isinstance(r, Exception))
                return {
                    "time": end - start,
                    "errors": errors,
                    "success": len(results) - errors
                }
            except Exception:
                end = time.perf_counter()
                return {
                    "time": end - start,
                    "errors": batch_size,
                    "success": 0
                }
        
        times = []
        total_errors = 0
        total_success = 0
        
        start_total = time.perf_counter()
        
        # Executa em lotes concorrentes
        remaining = scenario['iterations']
        while remaining > 0:
            batch_size = min(concurrency, remaining)
            result = await run_batch(batch_size)
            
            times.append(result['time'])
            total_errors += result['errors']
            total_success += result['success']
            remaining -= batch_size
        
        end_total = time.perf_counter()
        total_time = end_total - start_total
        
        return {
            "total_time": total_time,
            "avg_time": statistics.mean(times) if times else 0,
            "median_time": statistics.median(times) if times else 0,
            "min_time": min(times) if times else 0,
            "max_time": max(times) if times else 0,
            "std_dev": statistics.stdev(times) if len(times) > 1 else 0,
            "throughput": scenario['iterations'] / total_time,
            "errors": total_errors,
            "success_rate": total_success / scenario['iterations'] * 100 if scenario['iterations'] > 0 else 0
        }

    def format_performance_results(self, results: Dict) -> str:
        """Formata os resultados de performance"""
        return f"""
    ‚è±Ô∏è  Tempo Total: {results['total_time']:.3f}s
    üìà Throughput: {results['throughput']:.1f} ops/s
    ‚åÄ  Tempo M√©dio: {results['avg_time']*1000:.2f}ms
    üìä Tempo Mediano: {results['median_time']*1000:.2f}ms
    ‚¨áÔ∏è  Tempo M√≠n: {results['min_time']*1000:.2f}ms
    ‚¨ÜÔ∏è  Tempo M√°x: {results['max_time']*1000:.2f}ms
    üìè Desvio Padr√£o: {results['std_dev']*1000:.2f}ms
    ‚úÖ Taxa Sucesso: {results['success_rate']:.1f}%
    ‚ùå Erros: {results['errors']}
        """

    def calculate_improvement(self, sync_result: Dict, async_result: Dict) -> Dict:
        """Calcula melhorias de performance"""
        return {
            "total_time_improvement": ((sync_result['total_time'] - async_result['total_time']) / sync_result['total_time']) * 100,
            "throughput_improvement": ((async_result['throughput'] - sync_result['throughput']) / sync_result['throughput']) * 100,
            "avg_time_improvement": ((sync_result['avg_time'] - async_result['avg_time']) / sync_result['avg_time']) * 100
        }

    async def run_load_test(self):
        """Executa o teste de carga completo"""
        print("üöÄ Iniciando Teste de Carga: Sync vs Async Performance")
        print("=" * 70)
        
        scenarios = [
            self.get_light_load_scenario(),
            self.get_medium_load_scenario(),
            self.get_heavy_load_scenario()
        ]
        
        all_results = {}
        
        for scenario in scenarios:
            print(f"\nüéØ Cen√°rio: {scenario['name']} - {scenario['description']}")
            print(f"üìù Itera√ß√µes: {scenario['iterations']}")
            print("-" * 50)
            
            # Teste S√≠ncrono
            sync_results = self.measure_sync_performance(scenario)
            
            # Teste Ass√≠ncrono Sequencial
            async_results = await self.measure_async_performance(scenario)
            
            # Teste Ass√≠ncrono Concorrente
            concurrent_results = await self.measure_concurrent_async_performance(scenario, concurrency=10)
            
            # An√°lise dos resultados
            sync_vs_async = self.calculate_improvement(sync_results, async_results)
            sync_vs_concurrent = self.calculate_improvement(sync_results, concurrent_results)
            
            all_results[scenario['name']] = {
                "sync": sync_results,
                "async": async_results,
                "concurrent": concurrent_results,
                "improvements": {
                    "sync_vs_async": sync_vs_async,
                    "sync_vs_concurrent": sync_vs_concurrent
                }
            }
            
            # Exibe resultados
            print(f"\nüìä RESULTADOS - {scenario['name']}:")
            print("\nüîÑ SYNC:")
            print(self.format_performance_results(sync_results))
            
            print("\n‚ö° ASYNC:")
            print(self.format_performance_results(async_results))
            
            print("\nüöÄ ASYNC CONCURRENT:")
            print(self.format_performance_results(concurrent_results))
            
            print("\nüìà MELHORIAS:")
            print(f"  ‚Ä¢ Async vs Sync:")
            print(f"    - Tempo Total: {sync_vs_async['total_time_improvement']:+.1f}%")
            print(f"    - Throughput: {sync_vs_async['throughput_improvement']:+.1f}%")
            print(f"    - Tempo M√©dio: {sync_vs_async['avg_time_improvement']:+.1f}%")
            
            print(f"  ‚Ä¢ Concurrent vs Sync:")
            print(f"    - Tempo Total: {sync_vs_concurrent['total_time_improvement']:+.1f}%")
            print(f"    - Throughput: {sync_vs_concurrent['throughput_improvement']:+.1f}%")
            print(f"    - Tempo M√©dio: {sync_vs_concurrent['avg_time_improvement']:+.1f}%")
        
        # Resumo geral
        print("\n" + "=" * 70)
        print("üìã RESUMO GERAL")
        print("=" * 70)
        
        for scenario_name, results in all_results.items():
            sync_throughput = results['sync']['throughput']
            async_throughput = results['async']['throughput']
            concurrent_throughput = results['concurrent']['throughput']
            
            print(f"\nüéØ {scenario_name}:")
            print(f"  üìä Throughput (ops/s):")
            print(f"     - Sync: {sync_throughput:.1f}")
            print(f"     - Async: {async_throughput:.1f} ({results['improvements']['sync_vs_async']['throughput_improvement']:+.1f}%)")
            print(f"     - Concurrent: {concurrent_throughput:.1f} ({results['improvements']['sync_vs_concurrent']['throughput_improvement']:+.1f}%)")
        
        print(f"\n‚úÖ Teste de carga conclu√≠do!")
        return all_results


def main():
    """Fun√ß√£o principal"""
    load_test = LoadTest()
    
    try:
        # Executa o teste de carga
        results = asyncio.run(load_test.run_load_test())
        
        print(f"\nüéâ Todos os testes de carga foram executados com sucesso!")
        return True
        
    except Exception as e:
        print(f"\n‚ùå Erro durante o teste de carga: {e}")
        import traceback
        traceback.print_exc()
        return False


if __name__ == "__main__":
    success = main()
    exit(0 if success else 1)
